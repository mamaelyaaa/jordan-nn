# Нейронная сеть Джордана

## 1. Структура
- Входы: `k` значений
- Контекст: `n` значений  
- Скрытый слой: `m` нейронов
- Выходной слой: `n` нейронов

<br>

![jordan-nn](docs/jordan-nn-2.png)

<br>

## 2. Обозначения

- $\vec{x}$ — значения входа `[k × 1]`
- $\vec{c}$ — значения контекста `[n × 1]`
- $\vec{S_H}$ — состояния нейронов скрытого слоя `[m × 1]`
- $f_H(S)$ — функция активации нейронов скрытого слоя 
- $\vec{h}$ — значения выходов нейронов скрытого слоя `[m × 1]`
- $\vec{S_O}$ — состояния нейронов выходного слоя `[n × 1]`
- $f_O(S)$ — функция активации нейронов выходного слоя 
- $\vec{y}$ — значения выходов нейронов выходного слоя `[n × 1]`

<br>

**Матрицы весов**:

- $W_{IH}$ — веса связей между входами и нейронами скрытого слоя `[m × (k+1)]*`
- $W_{CH}$ — веса связей между контекстом и нейронами скрытого слоя `[m × n]`
- $W_{HO}$ — веса связей между нейронами скрытого и выходного слоёв `[n × (m+1)]*`

> \* – помимо кол-ва нейронов в слое учитывается фиктивный вход

<br>

## 3. Алгоритм прямого распространения сигнала

1. Расширяем вектор значений входов, добавляя единицу в начало вектора:

<br>

$$
\vec{\tilde{x}} = [1, \vec{x}]^{T}
$$

<br>

2. Рассчитываем состояния нейронов скрытого слоя:

<br>

$$
\vec{S_H} = W_{IH} \cdot \vec{\tilde{x}} + W_{CH} \cdot \vec{c}
$$

<br>

3. Рассчитываем значения выходов нейронов скрытого слоя:

<br>

$$
\vec{h} = f_H(\vec{S_H})
$$

<br>

4. Расширяем вектор значений выходов нейронов скрытого слоя, добавляя единицу в начало вектора:

<br>

$$
\vec{\tilde{h}} = [1, \vec{h}]^{T}
$$

<br>

5. Рассчитываем состояния нейронов выходного слоя:

<br>

$$
\vec{S_O} = W_{HO} \cdot \vec{\tilde{h}}
$$

<br>

6. Рассчитываем значения выходов нейронов выходного слоя:

<br>

$$
\vec{y} = f_O(\vec{S_O})
$$

<br>

## 4. Алгоритм расчёта поправок весов

> Технически, логика повторяет алгоритм обратного распространения ошибки, так как сеть представляет собой двуслойный перцептрон с добавлением контекста.

<br>

Пусть дан обучающий пример:
- $\vec{x}$ — значения входов примера
- $\vec{y_{пр}}$ — значения выходов примера

<br>

Значения весовых коэффициентов ($W_{IH}, W_{CH}, W_{HO}$), контекста ($\vec{c}$), а также коэффициента скорости обучения ($v$) должны быть определены заранее.

<br>

1. Рассчитываем выходы сети по алгоритму прямого распространения сигнала — $\vec{y}$ (см. раздел 3)

<br>

2. Рассчитываем вектор значений ошибок выходов сети относительно обучающего примера:

<br>

$$
\vec{\Delta} = \vec{y_{пр}} - \vec{y}
$$

<br>

3. Рассчитываем значения невязок нейронов выходного слоя:

<br>

$$
\vec{\delta_O} = \vec{\Delta} \odot f_O'(\vec{S_O})
$$

<br>

4. Рассчитываем значения невязок нейронов скрытого слоя:

<br>

$$
\vec{\delta_H} = (W_{HO}^T \cdot \vec{\delta_O}) \odot f_H'(\vec{S_H})
$$

<br>

5. Рассчитываются поправки весовых коэффициентов:

<br>

$$
\Delta{W_{IH}} = v \cdot (\vec{\delta_H} \cdot \vec{\tilde{x}}^{T})
$$

<br>

$$
\Delta{W_{CH}} = v \cdot (\vec{\delta_H} \cdot \vec{c}^{T})
$$

<br>

$$
\Delta{W_{HO}} = v \cdot (\vec{\delta_O} \cdot \vec{\tilde{h}}^{T})
$$

<br>

> $\vec{\tilde{x}}, \vec{\tilde{h}}$ — расширенные векторы значений входов примера и выходов нейронов скрытого слоя (см. пункты 3.1, 3.4)